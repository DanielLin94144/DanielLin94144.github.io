---
permalink: /
title: "About Guan-Ting (Daniel) Lin"
excerpt: "About Guan-Ting (Daniel) Lin"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Guan-Ting is currently a Ph.D. student at [Speech Processing and Machine Learning Lab](https://twitter.com/ntu_spml), [National Taiwan University (NTU)](https://www.ntu.edu.tw/), under the guidance of Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html). His research interest includes **Speech LLM** and **Spoken Language Understanding and Generation**. 

Guan-Ting has published 10+ papers at top-tier Speech/NLP conferences (ACL, EMNLP, ICASSP, INTERSPEECH, and IEEE SLT) as the first or co-first author. Notably, he won the **Best Paper Award** at IEEE SLT 2022 in Doha, Qatar. Additionally, he serves as an official reviewer for multiple top conferences (ICLR, ACL, EMNLP, ICASSP, etc.).

Guan-Ting has several **industrial research experience**:
He is currently an Student Researcher @ Google DeepMind (New York) in 2025 Spring. 
He is an incoming Research Scientist Intern @ Meta GenAI (Menlo Park) in 2025 Fall. 

Previously, in the summer of 2024, he interned with Amazon's AGI-Speech team in Seattle, USA, working on *Align-SLM*, which enhances end-to-end spoken language models with RL. In the summer of 2023, he worked with the Alexa Speech Recognition LM team on *paralinguisic-enhanced LLM* in Seattle, USA. In the summer of 2022, he collaborated with Amazon Alexa on Acoustic Event Classification in Cambridge, USA.

For more details, please see the **[[CV]](http://DanielLin94144.github.io/files/Guan_Ting_Lin_CV.pdf)**.

Beyond academia, he enjoys singing üé§, photography üì∑, and watching MLB games ‚öæÔ∏è.

<div style="display: flex; justify-content: center; gap: 20px;">
  <figure style="text-align: center;">
    <img src="../files/cropped-ntu_logo.png" alt="Image 1" width="100" height="100" style="border-radius: 50%; object-fit: cover;">
    <figcaption>NTU Âè∞ÁÅ£Â§ßÂ≠∏ (2021-Present)</figcaption>
  </figure>
  <figure style="text-align: center;">
    <img src="../files/cropped-alexa.png" alt="Image 2" width="100" height="100" style="border-radius: 50%; object-fit: cover;">
    <figcaption>Alexa AI (2022/2023 Summer)</figcaption>
  </figure>
  <figure style="text-align: center;">
    <img src="../files/cropped-amazon.png" alt="Image 3" width="100" height="100" style="border-radius: 50%; object-fit: cover;">
    <figcaption>Amazon AGI (2024 Summer)</figcaption>
  </figure>
  <figure style="text-align: center;">
    <img src="../files/cropped-deepmind.jpg" alt="Image 4" width="100" height="100" style="border-radius: 50%; object-fit: cover;">
    <figcaption>Google DeepMind (2025 Spring)</figcaption>
  </figure>
  <figure style="text-align: center;">
    <img src="../files/cropped-meta.jpg" alt="Image 5" width="100" height="100" style="border-radius: 50%; object-fit: cover;">
    <figcaption>Meta GenAI (2025 Fall)</figcaption>
  </figure>
</div>



Recent News üö®
======
* (11/2024) The preprint of [Align-SLM](https://arxiv.org/abs/2411.01834) is released, which is the first RLAIF framework for end-to-end textless spoken language models with state-of-the-art performance on SLM benchmarks!  
* (09/2024) [Continual TTA](https://arxiv.org/abs/2406.11064) and [Emphasized-Talk](https://arxiv.org/abs/2406.11065) are accepted by EMNLP 2024 (one as main and one as findings). 
* (05/2024) [Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations](https://arxiv.org/abs/2402.12786) is accepted by ACL 2024 as the main conference paper! 
* (01/2024) Received IEEE Signal Processing Society Travel Grant for participating ICASSP 2024! 
* (12/2023) Three papers are accepted by [ICASSP 2024](https://2024.ieeeicassp.org/) (one first-author and two co-author). See you in Seoul! 
* (02/2023) My internship work with Amazon Alexa is accepted by [ICASSP 2023](https://2023.ieeeicassp.org/)!
* (01/2023) Our paper "On the Utility of Self-supervised Models for Prosody-related Tasks", cooperating with Prof. Nigel Ward of UTEP, won the [Best Paper Award](https://slt2022.org/best-papers.php) of IEEE SLT 2022!
* (07/2022) Received ISCA Travel Grants for [Interspeech 2022](https://interspeech2022.org/).
* (06/2022) Two first-author papers are accepted at [Interspeech 2022](https://interspeech2022.org/).

Education
======
* **Ph.D.** in Communication Engineering, EECS, National Taiwan University
*[2021 - Present]*
  * Advisor: Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html)
  * GPA: 4.24/4.3; Ranking: 15/158
  * Transferred from M.S. program in Feb. 2023. 

Selected Publications & Preprints
======
(For full publication list, please see the [Google Scholar](https://scholar.google.com.tw/citations?user=gojQWGIAAAAJ&hl=en)).

**Speech/Text Large Language Models**
* **Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko\\
  *arXiv preprint*\\
  [paper](https://arxiv.org/abs/2411.01834)
* **Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations**\\
  <u>Guan-Ting Lin</u>, Cheng-Han Chiang, Hung-yi Lee\\
  *ACL 2024*\\
  [paper](https://arxiv.org/abs/2402.12786) / [data](https://github.com/DanielLin94144/StyleTalk)
* **Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, Ivan Bulyko\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2312.15316)
* **Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?**\\
  <u>Guan-Ting Lin</u>, Hung-yi Lee\\
  *EMNLP 2024 Findings*\\
  [paper](https://arxiv.org/abs/2406.11065) / [data](https://github.com/DanielLin94144/Emphasized-Talk)

**Self-supervised Speech Model**
* **On the Utility of Self-supervised Models for Prosody-related Task**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>,  Chi-Luen Feng<sub>(co-first)</sub>, Wei-Ping Huang, Yuan Tseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Nigel G. Ward\\
  *SLT 2022 (**Best Paper Award**)*\\
  [paper](https://arxiv.org/abs/2210.07185) / [code](https://github.com/JSALT-2022-SSL/superb-prosody)
* **Introducing Semantics into Speech Encoders**\\
  Derek Xu, Shuyan Dong, Changhan Wang, Suyoun Kim, Zhaojiang Lin, Akshat Shrivastava, Shang-Wen Li, Liang-Hsuan Tseng, Alexei Baevski, <u>Guan-Ting Lin</u>, Hung-yi Lee, Yizhou Sun, Wei Wang\\
  *ACL 2023*\\
  [paper](https://arxiv.org/abs/2211.08402)
* **SUPERB: Speech processing Universal PERformance Benchmark**\\
  Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, <u>Guan-Ting Lin</u>, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee\\
  *Interspeech 2021*\\
  [paper](https://arxiv.org/pdf/2105.01051) / [code](https://github.com/s3prl/s3prl)
* **Analyzing the Robustness of Unsupervised Speech Recognition**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Chan-Jan Hsu<sub>(co-first)</sub>, Da-Rong Liu, Hung-Yi Lee, Yu Tsao\\
  *ICASSP 2022*\\
  [paper](https://arxiv.org/pdf/2110.03509.pdf) / [code](https://github.com/Splend1d/wav2vec-u-patch)

**Spoken Language Understanding and Spoken Question Answering**
* **SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering**\\
  Chyi-Jiunn Lin, <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2401.13463)
* **Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks**\\
  Kevin Everson, Yile Gu, Huck Yang, Prashanth Gurunath Shivakumar, <u>Guan-Ting Lin</u>, Jari Kolehmainen, Ivan Bulyko, Ankur Gandhe, Shalini Ghosh, Wael Hamza, Hung-yi Lee, Ariya Rastrow, Andreas Stolcke\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2401.02921)
* **Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target**\\
  Guan-Wei Wu<sub>(co-first)</sub>, <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shang-Wen Li, Hung-yi Lee\\
  *Interspeech 2023*\\
  [paper](https://arxiv.org/abs/2305.18096)
* **DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering**\\
  <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Ho-Lam Chung, Shu-wen Yang, Hsuan-Jui Chen, Shuyan Dong, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *Interspeech 2022 (Poster)*\\
  [paper](https://arxiv.org/abs/2203.04911) / [code](https://github.com/DanielLin94144/DUAL-textless-SQA) 

**End-to-end ASR Test-time Adaptation**
* **Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech**\\
  <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Wei-Ping Huang<sub>(co-first)</sub>, Hung-yi Lee\\
  *EMNLP 2024*\\
  [paper](https://arxiv.org/abs/2406.11064)
* **Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition**\\
  <u>Guan-Ting Lin</u>, Shang-Wen Li, Hung-Yi Lee\\
  *Interspeech 2022 (Oral)*\\
  [paper](https://arxiv.org/abs/2203.14222) / [code](https://github.com/DanielLin94144/Test-time-adaptation-ASR-SUTA)


**Audio Event Classification**
* **Weight-sharing Supernet for Searching Specialized Acoustic Event Classification Networks Across Device Constraints**\\
  <u>Guan-Ting Lin</u>, Qingming Tang, Chieh-Chi Kao, Viktor Rozgic, Chao Wang\\
  *ICASSP 2023*\\
  [paper](https://arxiv.org/abs/2303.10351)


Experience
======
* **Student Researcher, Google DeepMind, New York, United States**\\
  *[2025/2 - Present]*
  * Gemini Speech Team.

* **Applied Scientist II Intern, Amazon AGI, Seattle, United States**\\
  *[2024/6 - 2024/9]*
  * AGI-Speech Research Team
  * End-to-end Spoken Language Models.

* **Applied Scientist II Intern, Amazon Alexa, Seattle, United States**\\
  *[2023/6 - 2023/9]*
  * Speech Recognition LM Research Team
  * Paralinguistics-enhanced Large Language Model on spoken dialogue.

* **Applied Scientist I Intern, Amazon Alexa, Cambridge, United States**\\
  *[2022/7 - 2022/10]*
  * Manager: Chieh-Chi Kao / Mentor: Qingming Tang
  * Develop Once-for-all Network Architecture Search techniques on audio event classification.

* **Visiting Reseacher, 8th JASLT Summer Workshop, Johns Hopkins University, Baltimore, United States**\\
  *[2022/6 - 2022/7]*
  * Work with Prof. [Nigel Ward](https://www.cs.utep.edu/nigel/) on [self-supervised pre-training for prosody](https://arxiv.org/abs/2210.07185). 

Award
======
* IEEE Signal Processing Society Travel Grant @ ICASSP 2024
* Best paper award @ IEEE SLT 2022
* NTU Elite Doctoral Scholarship
* GICE Elite Doctoral Scholarship with NVIDIA
* ISCA travel grant @ Interspeech 2022
* Appier top-tier conference scholarship
* Dean's list * 3 @ NTHU
* Phi Tau Phi Award @ NTHU
* The Zhu Shun Yi He Qin Scholarship @ NTHU

Academic Services
======
* **Official Reviewer**: ICLR'24'25, NeurIPS'24, ACL'24, EMNLP'24, NAACL'23'24, ICASSP'23'24, ISCSLP'22'23'24, COLING'25

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7Qw12O7m4eZyJ9EztFY7V_gZbGDuLrM-MTmcSbviX2w&cl=ffffff&w=a"></script>

