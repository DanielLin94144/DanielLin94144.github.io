---
permalink: /
title: "About Guan-Ting (Daniel) Lin"
excerpt: "About Guan-Ting (Daniel) Lin"
author_profile: true
hide_title: true
redirect_from: 
  - /about/
  - /about.html
---

{% include author-hero.html %}
Guan-Ting received his Ph.D. from the [Speech Processing and Machine Learning Lab](https://twitter.com/ntu_spml) at [National Taiwan University (NTU)](https://www.ntu.edu.tw/), where he was advised by Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html). His research interests include **Speech LLMs, Full-Duplex Interaction, Spoken Language Understanding / Generation, and Test-Time Adaptation for Automatic Speech Recognition**.

Guan-Ting has published **15+ first/co-first author** papers at top-tier Speech & NLP conferences (ACL, EMNLP, ICASSP, Interspeech, ASRU, SLT). Notably, he received the **Best Paper Award** at IEEE SLT 2022 in Doha, Qatar. He also regularly serves as an official reviewer for prestigious conferences/journals, including ICLR, NeurIPS, ACL, EMNLP, NAACL, TASLP, and ICASSP. He is recognized as [ICLR 2025 Notable Reviewer](https://iclr.cc/Conferences/2025/Reviewers).

He has been fortunate to gain extensive **research experience** through valuable opportunities:
- **[Meta Superintelligence Lab](https://ai.meta.com/) (2025 Fall):** Research Scientist Intern at the Voice Modeling Team in Menlo Park, USA, working with [Naoyuki Kanda](https://scholar.google.com/citations?user=j7q6qbAAAAAJ&hl=en) on full-duplex speech LLM.
- **[Google DeepMind](https://deepmind.google/) (2025 Spring):** Student Researcher at Gemini Speech team (New York City), collaborating with [Kartik Audhkhasi](https://research.google/people/kartikaudhkhasi/), [Soheil Khorram](https://scholar.google.com/citations?user=gdKu-GIAAAAJ&hl=en), and [Bhuvana Ramabhadran](https://sites.google.com/view/bhuvana-ramabhadran/home) to develop methods enhancing Gemini speech capabilities in low-resource languages.
- **[Amazon AGI](https://www.amazon.science/) (2024 Summer):** Applied Scientist Intern at Speech team in Seattle, USA (under [Ivan Bulyko](https://www.linkedin.com/in/ivan-bulyko-4641392/)'s team), working with [Prashanth Gurunath Shivakumar](https://www.amazon.science/author/prashanth-gurunath-shivakumar), [Yile Gu](https://www.linkedin.com/in/yilegu), and [Ankur Gandhe](https://www.linkedin.com/in/ankur-gandhe-15277a1a/) on *Align-SLM*, the first end-to-end spoken language model with reinforcement learning.
- **[Amazon Alexa AI](https://developer.amazon.com/en-US/alexa) (2023 Summer):** Applied Scientist Intern at Speech Recognition and LM team in Seattle, USA (under [Ivan Bulyko](https://www.linkedin.com/in/ivan-bulyko-4641392/)'s team), working with [Prashanth Gurunath Shivakumar](https://www.amazon.science/author/prashanth-gurunath-shivakumar) and [Andreas Stolcke](https://scholar.google.com/citations?user=NK36Tw0AAAAJ&hl=en) on a *paralinguistics-enhanced LLM*.
- **[Amazon Alexa AI](https://developer.amazon.com/en-US/alexa) (2022 Summer):** Applied Scientist Intern at in Cambridge, USA (under [Chao Wang](https://www.linkedin.com/in/chao-wang-0414968/)'s team), working with [Chieh-Chi Kao](https://www.linkedin.com/in/chieh-chi-kao/) and [Qingming Tang](https://home.ttic.edu/~qmtang/) on acoustic event classification using neural architecture search.

Open to discussing or collaborating on speech research‚Äîfeel free to reach out at `daniel094144[at]gmail[dot]com`.

Beyond academia, he enjoys singing üé§, photography üì∑, and watching MLB games ‚öæÔ∏è.

<div class="experience-grid">
  <figure>
    <div>
      <img src="../files/cropped-ntu_logo.png" alt="NTU">
    </div>
    <figcaption>NTU<br>(2021-Present)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-alexa.png" alt="Alexa AI">
    </div>
    <figcaption>Alexa AI<br>(2022/2023 Summer)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-amazon.png" alt="Amazon AGI">
    </div>
    <figcaption>Amazon AGI<br>(2024 Summer)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-deepmind.jpg" alt="Google DeepMind">
    </div>
    <figcaption>Google DeepMind<br>(2025 Spring)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-meta.jpg" alt="Meta Superintelligence Lab">
    </div>
    <figcaption>Meta Superintelligence Lab<br>(2025 Fall)</figcaption>
  </figure>
</div>


## Update

<ul class="news-list" id="news-list">
  <li><strong>2025/08</strong> Three papers accepted by <em>ASRU 2025</em> ‚Äî see you in Hawaii üèù</li>
  <li><strong>2025/05</strong> <a href="https://arxiv.org/abs/2411.01834">Align-SLM</a> accepted by <em>ACL 2025</em> ‚Äî see you in Vienna!</li>
  <li><strong>2025/03</strong> Released <em>Full-Duplex-Bench</em> ‚Äî the first benchmark for full-duplex spoken dialogue models.</li>
  <li><strong>2024/11</strong> Preprint of <em>Align-SLM</em> released ‚Äî first RLAIF framework for end-to-end textless SLMs.</li>
  <li><strong>2024/09</strong> <em>Continual TTA</em> & <em>Emphasized-Talk</em> accepted by EMNLP 2024 (main & findings).</li>
  <li><strong>2024/05</strong> <em>Advancing LLMs to Capture Speaking Styles</em> accepted by <em>ACL 2024</em>.</li>
  <li><strong>2024/01</strong> Received IEEE SPS Travel Grant for ICASSP 2024!</li>
  <li><strong>2023/12</strong> Three papers accepted by <em>ICASSP 2024</em> ‚Äî see you in Seoul!</li>
  <li><strong>2023/02</strong> Internship work with Amazon Alexa accepted by <em>ICASSP 2023</em>.</li>
  <li><strong>2023/01</strong> Paper with Prof. Nigel Ward won <strong>Best Paper Award</strong> at IEEE SLT 2022!</li>
  <li><strong>2022/07</strong> Received ISCA Travel Grant for Interspeech 2022.</li>
  <li><strong>2022/06</strong> Two first-author papers accepted at <em>Interspeech 2022</em>.</li>
</ul>
<button id="load-more-btn" class="btn btn--primary btn--centered" style="margin-top: 1em;">Show More</button>


## Education
* **Ph.D.** in Communication Engineering, EECS, National Taiwan University
*[2021/9 - 2025/12]*
  * Advisor: Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html)
  * Transferred from M.S. program in Feb. 2023. 

## Selected Publications & Preprints
(For full publication list, please see the [Google Scholar](https://scholar.google.com.tw/citations?user=gojQWGIAAAAJ&hl=en)).

<!-- Automated list removed to avoid duplication with manual list -->

<!-- Original List Backup
**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
... (rest of the manual list) -->

<!-- Old inline HTML content removed -->

**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench v1.5: Evaluating Overlap Handling for Full-Duplex Speech Models**\\
  <u>Guan-Ting Lin</u>, Shih-Yun Shan Kuan, Qirui Wang, Jiachen Lian, Tingle Li, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2507.23159) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities**\\
  <u>Guan-Ting Lin</u>, Jiachen Lian<sub>(co-second)</sub>, Tingle Li<sub>(co-second)</sub>, Qirui Wang<sub>(co-second)</sub>, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](http://arxiv.org/abs/2503.04721) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko\\
  *ACL 2025*\\
  [paper](https://arxiv.org/abs/2411.01834)
* **Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations**\\
  <u>Guan-Ting Lin</u>, Cheng-Han Chiang, Hung-yi Lee\\
  *ACL 2024*\\
  [paper](https://arxiv.org/abs/2402.12786) / [data](https://github.com/DanielLin94144/StyleTalk)
* **Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, Ivan Bulyko\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2312.15316)
* **Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?**\\
  <u>Guan-Ting Lin</u>, Hung-yi Lee\\
  *EMNLP 2024 Findings*\\
  [paper](https://arxiv.org/abs/2406.11065) / [data](https://github.com/DanielLin94144/Emphasized-Talk)

**[Self-supervised Speech Models]**\\
_Explore the utilities of self-supervised speech representations models_
* **On the Utility of Self-supervised Models for Prosody-related Task**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>,  Chi-Luen Feng<sub>(co-first)</sub>, Wei-Ping Huang, Yuan Tseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Nigel G. Ward\\
  *SLT 2022 (**Best Paper Award**)*\\
  [paper](https://arxiv.org/abs/2210.07185) / [code](https://github.com/JSALT-2022-SSL/superb-prosody)
* **Analyzing the Robustness of Unsupervised Speech Recognition**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Chan-Jan Hsu<sub>(co-first)</sub>, Da-Rong Liu, Hung-Yi Lee, Yu Tsao\\
  *ICASSP 2022*\\
  [paper](https://arxiv.org/pdf/2110.031009.pdf) / [code](https://github.com/Splend1d/wav2vec-u-patch)

**[Spoken Language Understanding and Spoken Question Answering]**\\
_End-to-end approaches to understand high-level semantic information in speech signals_
* **DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering**\\
  <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Ho-Lam Chung, Shu-wen Yang, Hsuan-Jui Chen, Shuyan Dong, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *Interspeech 2022*\\
  [paper](https://arxiv.org/abs/2203.04911) / [code](https://github.com/DanielLin94144/DUAL-textless-SQA) 
* **Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target**\\
  Guan-Wei Wu<sub>(co-first)</sub>, <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shang-Wen Li, Hung-yi Lee\\
  *Interspeech 2023*\\
  [paper](https://arxiv.org/abs/2305.18096)
* **SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering**\\
  Chyi-Jiunn Lin, <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2401.13463)

**[End-to-end ASR Test-time Adaptation]**\\
_Sample-dependent test-time adaptation to improve ASR on out-of-domain speech_

* **SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR**\\
  Wei-Ping Huang<sub>(co-first)</sub>, <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](https://arxiv.org/abs/2506.11121) / [code](https://github.com/hhhaaahhhaa/ASR-TTA)
* **Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech**\\
  <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Wei-Ping Huang<sub>(co-first)</sub>, Hung-yi Lee\\
  *EMNLP 2024*\\
  [paper](https://arxiv.org/abs/2406.11064) / [code](https://github.com/hhhaaahhhaa/Dynamic-SUTA)
* **Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition**\\
  <u>Guan-Ting Lin</u>, Shang-Wen Li, Hung-Yi Lee\\
  *Interspeech 2022 (Oral)*\\
  [paper](https://arxiv.org/abs/2203.14222) / [code](https://github.com/DanielLin94144/Test-time-adaptation-ASR-SUTA)

## Patents
* Inventor on a pending U.S. patent application in speech and language processing, filed by Google DeepMind (details confidential until publication)


## Award
* IEEE Signal Processing Society Travel Grant @ ICASSP 2024
* Best paper award @ IEEE SLT 2022
* NTU Elite Doctoral Scholarship
* GICE Elite Doctoral Scholarship with NVIDIA
* ISCA travel grant @ Interspeech 2022
* Appier top-tier conference scholarship
* Dean's list * 3 @ NTHU
* Phi Tau Phi Award @ NTHU
* The Zhu Shun Yi He Qin Scholarship @ NTHU

## Academic Services
* **Official Reviewer**: ICLR'24'25, NeurIPS'24'25, ACL'24'25, EMNLP'24, NAACL'23'24, ICASSP'23'24, ISCSLP'22'23'24, COLING'25

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7Qw12O7m4eZyJ9EztFY7V_gZbGDuLrM-MTmcSbviX2w&cl=ffffff&w=a"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
  var list = document.getElementById('news-list');
  if (!list) return;
  
  var items = list.querySelectorAll('li');
  var btn = document.getElementById('load-more-btn');
  var limit = 5;
  
  // Hide items beyond limit
  if (items.length > limit) {
    for (var i = limit; i < items.length; i++) {
      items[i].style.display = 'none';
      items[i].style.opacity = '0';
      items[i].style.transition = 'opacity 0.5s ease';
    }
  } else {
    if (btn) btn.style.display = 'none';
  }
  
  if (btn) {
    btn.addEventListener('click', function() {
      for (var i = limit; i < items.length; i++) {
        items[i].style.display = 'block'; // Or list-item
        // Trigger reflow
        void items[i].offsetWidth;
        items[i].style.opacity = '1';
      }
      this.style.display = 'none';
    });
  }
});
</script>

