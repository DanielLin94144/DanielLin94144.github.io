---
permalink: /
title: "Guan-Ting (Daniel) Lin"
excerpt: "Ph.D. from National Taiwan University"
author_profile: false
redirect_from: 
  - /about/
  - /about.html
---

<div class="profile-header">
  <h1 class="profile-name">Guan-Ting (Daniel) Lin</h1>
  <p class="profile-location">Taipei, Taiwan</p>
</div>

<div class="profile-grid">
  <div class="profile-bio">
    <p>
      Guan-Ting received his Ph.D. from the <a href="https://twitter.com/ntu_spml">Speech Processing and Machine Learning Lab</a> at <a href="https://www.ntu.edu.tw/">National Taiwan University (NTU)</a>, where he was advised by Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-yi Lee</a>. His research interests include <strong>Speech LLMs, Full-Duplex Interaction, Spoken Language Understanding / Generation, and Test-Time Adaptation for Automatic Speech Recognition</strong>.
    </p>
    <p>
      Guan-Ting has published <strong>15+ first/co-first author</strong> papers at top-tier Speech & NLP conferences (ACL, EMNLP, ICASSP, Interspeech, ASRU, SLT). Notably, he received the <strong>Best Paper Award</strong> at IEEE SLT 2022 in Doha, Qatar. He also regularly serves as an official reviewer for prestigious conferences/journals, including ICLR, NeurIPS, ACL, EMNLP, NAACL, TASLP, and ICASSP. He is recognized as <a href="https://iclr.cc/Conferences/2025/Reviewers">ICLR 2025 Notable Reviewer</a>.
    </p>
    <div class="hero__social" style="justify-content: flex-start; margin-top: 1em;">
        <a href="http://DanielLin94144.github.io/files/Guan_Ting_Lin_CV.pdf" class="btn btn--primary"><i class="fas fa-file-alt"></i> CV</a>
        <a href="https://scholar.google.com/citations?user=gojQWGIAAAAJ" class="btn btn--google-plus"><i class="fab fa-google-scholar"></i> Scholar</a>
        <a href="https://www.linkedin.com/in/guan-ting-lin/" class="btn btn--linkedin"><i class="fab fa-linkedin"></i> LinkedIn</a>
        <a href="https://github.com/DanielLin94144" class="btn btn--github"><i class="fab fa-github"></i> GitHub</a>
    </div>
  </div>

  <div class="profile-image-container">
    <img src="../files/profile.jpg" alt="Guan-Ting (Daniel) Lin">
    <figcaption>Dumbo, USA (Feb. 2025)</figcaption>
  </div>
</div>

Open to discussing or collaborating on speech research‚Äîfeel free to reach out at `daniel094144[at]gmail[dot]com`.

Beyond academia, he enjoys singing üé§, photography üì∑, and watching MLB games ‚öæÔ∏è.

<div class="experience-grid">
  <figure>
    <div>
      <img src="../files/cropped-ntu_logo.png" alt="NTU">
    </div>
    <figcaption>NTU<br>(2021-Present)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-alexa.png" alt="Alexa AI">
    </div>
    <figcaption>Alexa AI<br>(2022/2023 Summer)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-amazon.png" alt="Amazon AGI">
    </div>
    <figcaption>Amazon AGI<br>(2024 Summer)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-deepmind.jpg" alt="Google DeepMind">
    </div>
    <figcaption>Google DeepMind<br>(2025 Spring)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-meta.jpg" alt="Meta Superintelligence Lab">
    </div>
    <figcaption>Meta Superintelligence Lab<br>(2025 Fall)</figcaption>
  </figure>
</div>


## Update

<ul class="news-list" id="news-list">
  <li><strong>2025/08</strong> Three papers accepted by <em>ASRU 2025</em> ‚Äî see you in Hawaii üèù</li>
  <li><strong>2025/05</strong> <a href="https://arxiv.org/abs/2411.01834">Align-SLM</a> accepted by <em>ACL 2025</em> ‚Äî see you in Vienna!</li>
  <li><strong>2025/03</strong> Released <em>Full-Duplex-Bench</em> ‚Äî the first benchmark for full-duplex spoken dialogue models.</li>
  <li><strong>2024/11</strong> Preprint of <em>Align-SLM</em> released ‚Äî first RLAIF framework for end-to-end textless SLMs.</li>
  <li><strong>2024/09</strong> <em>Continual TTA</em> & <em>Emphasized-Talk</em> accepted by EMNLP 2024 (main & findings).</li>
  <li><strong>2024/05</strong> <em>Advancing LLMs to Capture Speaking Styles</em> accepted by <em>ACL 2024</em>.</li>
  <li><strong>2024/01</strong> Received IEEE SPS Travel Grant for ICASSP 2024!</li>
  <li><strong>2023/12</strong> Three papers accepted by <em>ICASSP 2024</em> ‚Äî see you in Seoul!</li>
  <li><strong>2023/02</strong> Internship work with Amazon Alexa accepted by <em>ICASSP 2023</em>.</li>
  <li><strong>2023/01</strong> Paper with Prof. Nigel Ward won <strong>Best Paper Award</strong> at IEEE SLT 2022!</li>
  <li><strong>2022/07</strong> Received ISCA Travel Grant for Interspeech 2022.</li>
  <li><strong>2022/06</strong> Two first-author papers accepted at <em>Interspeech 2022</em>.</li>
</ul>



## Education
* **Ph.D.** in Communication Engineering, EECS, National Taiwan University
*[2021/9 - 2025/12]*
  * Advisor: Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html)
  * Transferred from M.S. program in Feb. 2023. 

## Selected Publications & Preprints
(For full publication list, please see the [Google Scholar](https://scholar.google.com.tw/citations?user=gojQWGIAAAAJ&hl=en)).

<!-- Automated list removed to avoid duplication with manual list -->

<!-- Original List Backup
**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
... (rest of the manual list) -->

<!-- Old inline HTML content removed -->

**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench v1.5: Evaluating Overlap Handling for Full-Duplex Speech Models**\\
  <u>Guan-Ting Lin</u>, Shih-Yun Shan Kuan, Qirui Wang, Jiachen Lian, Tingle Li, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2507.23159) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities**\\
  <u>Guan-Ting Lin</u>, Jiachen Lian<sub>(co-second)</sub>, Tingle Li<sub>(co-second)</sub>, Qirui Wang<sub>(co-second)</sub>, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](http://arxiv.org/abs/2503.04721) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko\\
  *ACL 2025*\\
  [paper](https://arxiv.org/abs/2411.01834)
* **Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations**\\
  <u>Guan-Ting Lin</u>, Cheng-Han Chiang, Hung-yi Lee\\
  *ACL 2024*\\
  [paper](https://arxiv.org/abs/2402.12786) / [data](https://github.com/DanielLin94144/StyleTalk)
* **Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, Ivan Bulyko\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2312.15316)
* **Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?**\\
  <u>Guan-Ting Lin</u>, Hung-yi Lee\\
  *EMNLP 2024 Findings*\\
  [paper](https://arxiv.org/abs/2406.11065) / [data](https://github.com/DanielLin94144/Emphasized-Talk)

**[Self-supervised Speech Models]**\\
_Explore the utilities of self-supervised speech representations models_
* **On the Utility of Self-supervised Models for Prosody-related Task**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>,  Chi-Luen Feng<sub>(co-first)</sub>, Wei-Ping Huang, Yuan Tseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Nigel G. Ward\\
  *SLT 2022 (**Best Paper Award**)*\\
  [paper](https://arxiv.org/abs/2210.07185) / [code](https://github.com/JSALT-2022-SSL/superb-prosody)
* **Analyzing the Robustness of Unsupervised Speech Recognition**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Chan-Jan Hsu<sub>(co-first)</sub>, Da-Rong Liu, Hung-Yi Lee, Yu Tsao\\
  *ICASSP 2022*\\
  [paper](https://arxiv.org/pdf/2110.031009.pdf) / [code](https://github.com/Splend1d/wav2vec-u-patch)

**[Spoken Language Understanding and Spoken Question Answering]**\\
_End-to-end approaches to understand high-level semantic information in speech signals_
* **DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering**\\
  <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Ho-Lam Chung, Shu-wen Yang, Hsuan-Jui Chen, Shuyan Dong, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *Interspeech 2022*\\
  [paper](https://arxiv.org/abs/2203.04911) / [code](https://github.com/DanielLin94144/DUAL-textless-SQA) 
* **Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target**\\
  Guan-Wei Wu<sub>(co-first)</sub>, <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shang-Wen Li, Hung-yi Lee\\
  *Interspeech 2023*\\
  [paper](https://arxiv.org/abs/2305.18096)
* **SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering**\\
  Chyi-Jiunn Lin, <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2401.13463)

**[End-to-end ASR Test-time Adaptation]**\\
_Sample-dependent test-time adaptation to improve ASR on out-of-domain speech_

* **SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR**\\
  Wei-Ping Huang<sub>(co-first)</sub>, <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](https://arxiv.org/abs/2506.11121) / [code](https://github.com/hhhaaahhhaa/ASR-TTA)
* **Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech**\\
  <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Wei-Ping Huang<sub>(co-first)</sub>, Hung-yi Lee\\
  *EMNLP 2024*\\
  [paper](https://arxiv.org/abs/2406.11064) / [code](https://github.com/hhhaaahhhaa/Dynamic-SUTA)
* **Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition**\\
  <u>Guan-Ting Lin</u>, Shang-Wen Li, Hung-Yi Lee\\
  *Interspeech 2022 (Oral)*\\
  [paper](https://arxiv.org/abs/2203.14222) / [code](https://github.com/DanielLin94144/Test-time-adaptation-ASR-SUTA)

## Patents
* Inventor on a pending U.S. patent application in speech and language processing, filed by Google DeepMind (details confidential until publication)


## Award
* IEEE Signal Processing Society Travel Grant @ ICASSP 2024
* Best paper award @ IEEE SLT 2022
* NTU Elite Doctoral Scholarship
* GICE Elite Doctoral Scholarship with NVIDIA
* ISCA travel grant @ Interspeech 2022
* Appier top-tier conference scholarship
* Dean's list * 3 @ NTHU
* Phi Tau Phi Award @ NTHU
* The Zhu Shun Yi He Qin Scholarship @ NTHU

## Academic Services
* **Official Reviewer**: ICLR'24'25, NeurIPS'24'25, ACL'24'25, EMNLP'24, NAACL'23'24, ICASSP'23'24, ISCSLP'22'23'24, COLING'25

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7Qw12O7m4eZyJ9EztFY7V_gZbGDuLrM-MTmcSbviX2w&cl=ffffff&w=a"></script>



