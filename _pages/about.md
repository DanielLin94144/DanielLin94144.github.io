---
permalink: /
title: "Guan-Ting (Daniel) Lin"
excerpt: "Ph.D. from National Taiwan University"
author_profile: false
hide_title: true
redirect_from: 
  - /about/
  - /about.html
---

<div class="profile-header">
  <h1 class="profile-name">Guan-Ting (Daniel) Lin</h1>
  <p class="profile-location">Taipei, Taiwan</p>
</div>

<div class="profile-grid">
  <div class="profile-bio">
    <p>
      Guan-Ting received his Ph.D. from the <a href="https://twitter.com/ntu_spml">Speech Processing and Machine Learning Lab</a> at <a href="https://www.ntu.edu.tw/">National Taiwan University (NTU)</a>, advised by Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-yi Lee</a>. His research focuses on <strong>Speech LLMs, Full-Duplex Interaction, Spoken Language Understanding/Generation, and Test-Time Adaptation for ASR</strong>.
    </p>
    <p>
      He has published <strong>15+ first/co-first author</strong> papers at top-tier conferences (ACL, EMNLP, ICASSP, Interspeech, ASRU, SLT) and received the <strong>Best Paper Award</strong> at IEEE SLT 2022. He serves as a reviewer for ICLR, NeurIPS, ACL, EMNLP, and is recognized as <a href="https://iclr.cc/Conferences/2025/Reviewers">ICLR 2025 Notable Reviewer</a>.
    </p>
    <p><strong>Research Experience:</strong></p>
    <ul>
      <li><strong><a href="https://ai.meta.com/">Meta Superintelligence Lab</a> (2025 Fall):</strong> Research Scientist Intern at the Voice Modeling Team in Menlo Park, USA, working with <a href="https://scholar.google.com/citations?user=j7q6qbAAAAAJ&hl=en">Naoyuki Kanda</a> on full-duplex speech LLM.</li>
      <li><strong><a href="https://deepmind.google/">Google DeepMind</a> (2025 Spring):</strong> Student Researcher at Gemini Speech team (New York City), collaborating with <a href="https://research.google/people/kartikaudhkhasi/">Kartik Audhkhasi</a>, <a href="https://scholar.google.com/citations?user=gdKu-GIAAAAJ&hl=en">Soheil Khorram</a>, and <a href="https://sites.google.com/view/bhuvana-ramabhadran/home">Bhuvana Ramabhadran</a> on Gemini low-resource speech.</li>
      <li><strong><a href="https://www.amazon.science/">Amazon AGI</a> (2024 Summer):</strong> Applied Scientist Intern at Speech team in Seattle, USA (under <a href="https://www.linkedin.com/in/ivan-bulyko-4641392/">Ivan Bulyko</a>'s team), working with <a href="https://www.amazon.science/author/prashanth-gurunath-shivakumar">Prashanth Gurunath Shivakumar</a>, <a href="https://www.linkedin.com/in/yilegu">Yile Gu</a>, and <a href="https://www.linkedin.com/in/ankur-gandhe-15277a1a/">Ankur Gandhe</a> on <em>Align-SLM</em> (RLAIF for end-to-end SLMs).</li>
      <li><strong><a href="https://developer.amazon.com/en-US/alexa">Amazon Alexa AI</a> (2023 Summer):</strong> Applied Scientist Intern at Speech Recognition and LM team in Seattle, USA (under <a href="https://www.linkedin.com/in/ivan-bulyko-4641392/">Ivan Bulyko</a>'s team), working with <a href="https://www.amazon.science/author/prashanth-gurunath-shivakumar">Prashanth Gurunath Shivakumar</a> and <a href="https://scholar.google.com/citations?user=NK36Tw0AAAAJ&hl=en">Andreas Stolcke</a> on paralinguistics-enhanced LLM.</li>
      <li><strong><a href="https://developer.amazon.com/en-US/alexa">Amazon Alexa AI</a> (2022 Summer):</strong> Applied Scientist Intern in Cambridge, USA (under <a href="https://www.linkedin.com/in/chao-wang-0414968/">Chao Wang</a>'s team), working with <a href="https://www.linkedin.com/in/chieh-chi-kao/">Chieh-Chi Kao</a> and <a href="https://home.ttic.edu/~qmtang/">Qingming Tang</a> on neural architecture search for audio.</li>
    </ul>
  </div>

  <div class="profile-image-container">
    <img src="../files/profile.jpg" alt="Guan-Ting (Daniel) Lin">
    <figcaption>Dumbo, USA (Feb. 2025)</figcaption>
    
    <div class="hero__social" style="display: flex; justify-content: center; gap: 1.2em; margin-top: 1.5em; flex-wrap: wrap;">
      <a href="mailto:daniel094144@gmail.com" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="Email"><i class="fas fa-envelope"></i></a>
      <a href="https://www.linkedin.com/in/guan-ting-lin/" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
      <a href="https://x.com/GTL094144" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="X (Twitter)"><i class="fab fa-twitter"></i></a>
      <a href="https://github.com/DanielLin94144" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="GitHub"><i class="fab fa-github"></i></a>
      <a href="https://scholar.google.com/citations?user=gojQWGIAAAAJ" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
      <a href="http://DanielLin94144.github.io/files/Guan_Ting_Lin_CV.pdf" style="color: inherit; font-size: 1.8em; text-decoration: none;" title="CV"><i class="fas fa-file-alt"></i></a>
    </div>
  </div>
</div>

<p style="margin-top: 0;">Open to discussing or collaborating on speech research‚Äîfeel free to reach out at <code>daniel094144[at]gmail[dot]com</code>.</p>

<p>Beyond academia, he enjoys singing üé§, photography üì∑, and watching MLB games ‚öæÔ∏è.</p>

<div class="experience-grid">
  <figure>
    <div>
      <img src="../files/cropped-ntu_logo.png" alt="NTU">
    </div>
    <figcaption>NTU<br>(2021-Present)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-alexa.png" alt="Alexa AI">
    </div>
    <figcaption>Alexa AI<br>(2022/2023)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-amazon.png" alt="Amazon AGI">
    </div>
    <figcaption>Amazon AGI<br>(2024)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-deepmind.jpg" alt="Google DeepMind">
    </div>
    <figcaption>Google DeepMind<br>(2025 Spring)</figcaption>
  </figure>
  
  <figure>
    <div>
      <img src="../files/cropped-meta.jpg" alt="Meta">
    </div>
    <figcaption>Meta<br>(2025 Fall)</figcaption>
  </figure>
</div>


## Update

<ul class="news-list" id="news-list">
  <li><strong>2025/08</strong> Three papers accepted by ASRU 2025 ‚Äî see you in Hawaii üèù</li>
  <li><strong>2025/05</strong> <a href="https://arxiv.org/abs/2411.01834">Align-SLM</a> accepted by ACL 2025 ‚Äî see you in Vienna!</li>
  <li><strong>2025/03</strong> Released Full-Duplex-Bench ‚Äî the first benchmark for full-duplex spoken dialogue models.</li>
  <li><strong>2024/11</strong> Preprint of Align-SLM released ‚Äî first RLAIF framework for end-to-end textless SLMs.</li>
  <li><strong>2024/09</strong> Continual TTA & Emphasized-Talk accepted by EMNLP 2024 (main & findings).</li>
  <li><strong>2024/05</strong> Advancing LLMs to Capture Speaking Styles accepted by ACL 2024.</li>
  <li><strong>2024/01</strong> Received IEEE SPS Travel Grant for ICASSP 2024!</li>
  <li><strong>2023/12</strong> Three papers accepted by ICASSP 2024 ‚Äî see you in Seoul!</li>
  <li><strong>2023/02</strong> Internship work with Amazon Alexa accepted by ICASSP 2023.</li>
  <li><strong>2023/01</strong> Paper with Prof. Nigel Ward won <strong>Best Paper Award</strong> at IEEE SLT 2022!</li>
  <li><strong>2022/07</strong> Received ISCA Travel Grant for Interspeech 2022.</li>
  <li><strong>2022/06</strong> Two first-author papers accepted at Interspeech 2022.</li>
</ul>



## Education
* **Ph.D.** in Communication Engineering, EECS, National Taiwan University
*[2021/9 - 2025/12]*
  * Advisor: Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.html)
  * Transferred from M.S. program in Feb. 2023. 

## Selected Publications & Preprints
(For full publication list, please see the [Google Scholar](https://scholar.google.com.tw/citations?user=gojQWGIAAAAJ&hl=en)).

<!-- Automated list removed to avoid duplication with manual list -->

<!-- Original List Backup
**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
... (rest of the manual list) -->

<!-- Old inline HTML content removed -->

**[Speech/Text Large Language Models]**\\
_Speech understanding and generation toward human-like spoken dialogue_
* **Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shih-Yun Shan Kuan<sub>(co-first)</sub>, Jiatong Shi, Kai-Wei Chang, Siddhant Arora, Shinji Watanabe, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2510.07838) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench v1.5: Evaluating Overlap Handling for Full-Duplex Speech Models**\\
  <u>Guan-Ting Lin</u>, Shih-Yun Shan Kuan, Qirui Wang, Jiachen Lian, Tingle Li, Hung-yi Lee\\
  *Arxiv 2025*\\
  [paper](https://arxiv.org/abs/2507.23159) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities**\\
  <u>Guan-Ting Lin</u>, Jiachen Lian<sub>(co-second)</sub>, Tingle Li<sub>(co-second)</sub>, Qirui Wang<sub>(co-second)</sub>, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](http://arxiv.org/abs/2503.04721) / [code](https://github.com/DanielLin94144/Full-Duplex-Bench)
* **Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko\\
  *ACL 2025*\\
  [paper](https://arxiv.org/abs/2411.01834)
* **Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations**\\
  <u>Guan-Ting Lin</u>, Cheng-Han Chiang, Hung-yi Lee\\
  *ACL 2024*\\
  [paper](https://arxiv.org/abs/2402.12786) / [data](https://github.com/DanielLin94144/StyleTalk)
* **Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue**\\
  <u>Guan-Ting Lin</u>, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, Ivan Bulyko\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2312.15316)
* **Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?**\\
  <u>Guan-Ting Lin</u>, Hung-yi Lee\\
  *EMNLP 2024 Findings*\\
  [paper](https://arxiv.org/abs/2406.11065) / [data](https://github.com/DanielLin94144/Emphasized-Talk)

**[Self-supervised Speech Models]**\\
_Explore the utilities of self-supervised speech representations models_
* **On the Utility of Self-supervised Models for Prosody-related Task**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>,  Chi-Luen Feng<sub>(co-first)</sub>, Wei-Ping Huang, Yuan Tseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Nigel G. Ward\\
  *SLT 2022 (**Best Paper Award**)*\\
  [paper](https://arxiv.org/abs/2210.07185) / [code](https://github.com/JSALT-2022-SSL/superb-prosody)
* **Analyzing the Robustness of Unsupervised Speech Recognition**\\
  <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Chan-Jan Hsu<sub>(co-first)</sub>, Da-Rong Liu, Hung-Yi Lee, Yu Tsao\\
  *ICASSP 2022*\\
  [paper](https://arxiv.org/pdf/2110.031009.pdf) / [code](https://github.com/Splend1d/wav2vec-u-patch)

**[Spoken Language Understanding and Spoken Question Answering]**\\
_End-to-end approaches to understand high-level semantic information in speech signals_
* **DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering**\\
  <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Ho-Lam Chung, Shu-wen Yang, Hsuan-Jui Chen, Shuyan Dong, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *Interspeech 2022*\\
  [paper](https://arxiv.org/abs/2203.04911) / [code](https://github.com/DanielLin94144/DUAL-textless-SQA) 
* **Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target**\\
  Guan-Wei Wu<sub>(co-first)</sub>, <u>Guan-Ting Lin</u><sub>(co-first)</sub>, Shang-Wen Li, Hung-yi Lee\\
  *Interspeech 2023*\\
  [paper](https://arxiv.org/abs/2305.18096)
* **SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering**\\
  Chyi-Jiunn Lin, <u>Guan-Ting Lin</u>, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee\\
  *ICASSP 2024*\\
  [paper](https://arxiv.org/abs/2401.13463)

**[End-to-end ASR Test-time Adaptation]**\\
_Sample-dependent test-time adaptation to improve ASR on out-of-domain speech_

* **SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR**\\
  Wei-Ping Huang<sub>(co-first)</sub>, <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Hung-yi Lee\\
  *ASRU 2025*\\
  [paper](https://arxiv.org/abs/2506.11121) / [code](https://github.com/hhhaaahhhaa/ASR-TTA)
* **Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech**\\
  <u>Guan-Ting Lin<sub>(co-first)</sub></u>, Wei-Ping Huang<sub>(co-first)</sub>, Hung-yi Lee\\
  *EMNLP 2024*\\
  [paper](https://arxiv.org/abs/2406.11064) / [code](https://github.com/hhhaaahhhaa/Dynamic-SUTA)
* **Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition**\\
  <u>Guan-Ting Lin</u>, Shang-Wen Li, Hung-Yi Lee\\
  *Interspeech 2022*\\
  [paper](https://arxiv.org/abs/2203.14222) / [code](https://github.com/DanielLin94144/Test-time-adaptation-ASR-SUTA)

## Patents
* Inventor on a pending U.S. patent application in speech and language processing, filed by Google DeepMind (details confidential until publication)


## Award
* IEEE Signal Processing Society Travel Grant @ ICASSP 2024
* Best paper award @ IEEE SLT 2022
* NTU Elite Doctoral Scholarship
* GICE Elite Doctoral Scholarship with NVIDIA
* ISCA travel grant @ Interspeech 2022
* Appier top-tier conference scholarship
* Dean's list * 3 @ NTHU
* Phi Tau Phi Award @ NTHU
* The Zhu Shun Yi He Qin Scholarship @ NTHU

## Academic Services
* **Official Reviewer**: ICLR'24'25, NeurIPS'24'25, ACL'24'25, EMNLP'24, NAACL'23'24, ICASSP'23'24, ISCSLP'22'23'24, COLING'25

<div style="width: 250px; margin: 2em auto; display: block;">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7Qw12O7m4eZyJ9EztFY7V_gZbGDuLrM-MTmcSbviX2w&cl=ffffff&w=a"></script>
</div>





